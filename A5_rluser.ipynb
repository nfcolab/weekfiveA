{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae418086",
   "metadata": {},
   "source": [
    "# Week 5, Task 1\n",
    "This notebook covers the assignment for week 5. It uses a reinforcement learning model that needs to learn to press a sequence of buttons (in the correct order), while choosing the correct speed-accuracy tradeoff for each button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import who_rl\n",
    "\n",
    "# We will use these libraries later in the notebook, but let's import everything here.\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da77d0a",
   "metadata": {},
   "source": [
    "The file contains a function `simulate_who_task`, which takes a `ui` object as its parameter. That UI must have three elements labelled \"e1\", \"e2\", and \"e3\". The transition and reward functions of the RL agent are created so that the agent must learn a pointing policy, where it gets a positive reward when pressing \"e3\", assuming it has first pressed \"e2\" and \"e1\" before that. So, it must discover the sequence of pressing elements in the correct order, as well as the correct speed-accuracy tradeoff for doing that. The agent gets a positive reward of `+1` at pressing \"e3\" (correctly), and otherwise `0`. In addition, a number is substracted from this reward, equal to the movement time if the current pointing movement. The optimal policy is therefore one that minimises movement time, while still quickly hitting the targets in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccfb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ui\n",
    "# Here is the example ui, note the elements named \"e1\" 2 and 3.\n",
    "ui.visualise_UI(ui.big_ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the model accomplishing the task. It takes some time to train the models via trial and error.\n",
    "path, mt = who_rl.simulate_who_task(ui.big_ui)\n",
    "print(\"The path taken by the model is\", repr(path))\n",
    "print(\"Movement times and speed-accuracy tradeoffs associated with this path are\", repr(mt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe2e2b",
   "metadata": {},
   "source": [
    "When the path contains an element name, the pointing action is a hit. When it contains a coordinate, the pointing was a miss. The second returned array contains the movement time, in seconds, passed since the start of the task. The total movement time is the last value. Associated with each movement time is the selected speed-accuracy tradeoff (MA = Maximum accuracy; A = Accuracy; B = Balance; S = Speed; MS = Maximum speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the following:\n",
    "path, mt = who_rl.simulate_who_task(ui.big_ui)\n",
    "ui.visualise_UI(ui.big_ui, path = path, annotate = mt)\n",
    "print(\"Task ended with mt =\", mt[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or just use the handy function for it.\n",
    "who_rl.visualise_who_task(ui.big_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd44489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we might want to have an average. This will take some time, so we'll report the progress\n",
    "who_rl.visualise_who_task(ui.big_ui) # first get an example visualisation\n",
    "results = []\n",
    "for i in range(10):\n",
    "    print(\"Run\", i+1, \"out of 10\")\n",
    "    path, mt = who_rl.simulate_who_task(ui.big_ui)\n",
    "    results.append(mt[-1][0])\n",
    "print(\"Average mt =\", np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we adapt the optimal pointing policy to the structure of the task?\n",
    "new_ui = copy.deepcopy(ui.big_ui)\n",
    "new_ui.modify_element(\"e2\", \"x_size\", 400)\n",
    "new_ui.modify_element(\"e2\", \"y_size\", 400)\n",
    "# Need to remove some elements to make space\n",
    "del new_ui.elements[\"c2\"]\n",
    "who_rl.visualise_who_task(new_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make e3 small. Maybe not small enough yet?\n",
    "import numpy as np\n",
    "new_ui = copy.deepcopy(ui.big_ui)\n",
    "new_ui.modify_element(\"e2\", \"x_size\", 400)\n",
    "new_ui.modify_element(\"e2\", \"y_size\", 400)\n",
    "new_ui.modify_element(\"e3\", \"x_size\", 10)\n",
    "new_ui.modify_element(\"e3\", \"y_size\", 10)\n",
    "# Need to remove some elements to make space\n",
    "del new_ui.elements[\"c2\"]\n",
    "who_rl.visualise_who_task(new_ui)\n",
    "results = []\n",
    "for i in range(10):\n",
    "    print(\"Run\", i+1, \"out of 10\")\n",
    "    path, mt = who_rl.simulate_who_task(new_ui)\n",
    "    results.append(mt[-1][0])\n",
    "print(\"Average mt =\", np.mean(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9bfd14",
   "metadata": {},
   "source": [
    "We can see from the above that the model does not really mind approaching the target with maximum speed (this is called a ballistic movement), and then \"homing in\" with a precise pointing movement. What if we, however, penalise for missing the target, meaning that the task requires one singular movement from the start to the target?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are increasing the training time here to make sure that the model learns.\n",
    "who_rl.visualise_who_task(new_ui, miss_penalty = -1, episodes = 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307633d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to change the pointing ability of the simulated user. This is a very able user.\n",
    "who_rl.visualise_who_task(ui.big_ui, who_alpha = 0.05, episodes = 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ebffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new element, e4. Use c4 as a template, and then remove that element.\n",
    "new_ui = copy.deepcopy(ui.big_ui)\n",
    "new_element = copy.deepcopy(new_ui.elements[\"c3\"])\n",
    "new_element.name = \"e4\"\n",
    "new_ui.elements[\"e4\"] = new_element\n",
    "del new_ui.elements[\"c3\"]\n",
    "who_rl.visualise_who_task(new_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da96483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the same task, but now hitting e3 is more rewarding.\n",
    "new_ui = copy.deepcopy(ui.big_ui)\n",
    "new_element = copy.deepcopy(new_ui.elements[\"c3\"])\n",
    "new_element.name = \"e4\"\n",
    "new_ui.elements[\"e4\"] = new_element\n",
    "del new_ui.elements[\"c3\"]\n",
    "who_rl.visualise_who_task(new_ui, e3_reward = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
